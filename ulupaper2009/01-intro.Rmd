# Model Implementation ( @ulu2009or- Part2.1 )

## Importing the required library

```{r}
library(tidyverse)
```
## Model parameters definition

$\theta:$

the Net Present Value of Stream of benefits

-   If she decides to reject the technology, she receives nothing and no longer gathers information about the technology.

-   If she decides to *adopt* the technology, she pays a fixed adoption cost K and receives a net expected benefit:

$$\int_{\theta} \theta \pi(\theta)d\theta - K$$

$\theta$ has the density $\pi$.

-   Now, if consumer choose to gather additional information, she pays *c* *in that period* and observe the signal x, drawn with likelihood function

$$L(x|\theta)$$

Having observed signal x, the consumer then updates her prior $\pi$ using Baye's rule:

$$\prod(\theta;\pi,x) = \frac{L(x|\theta)\pi(\theta)}{f(x;\pi)} $$

where the f(x;$\pi$) is the predictive ditribution for signals x,

$$f(x;\pi) =\int_{\theta} L(x|\theta)\pi(\theta)d\theta$$

The consumer then continues into the next period, starting with a new prior distribution that is equal to her posterior distribution from this stage.


Because our dynamic programming state variable is distribution itself , we will frequently supress the domain of the distribution and write the posterior as

The consumer's optimal value function with k periods remaining

$$v^{*}_{0}(\pi) = 0$$
$$v^{*}_{k}(\pi)=max(0,\int_{\theta}\theta\pi(\theta) \theta -K , -c + \delta E[v^{*}_{k-1}(\Pi(\pi,x))])$$

where $\delta$ (0<$\delta$<1) is the discount factor and the expectation of the next period value function is taken over all possible random signals.


$$E[v^{*}_{k-1}(\Pi(\pi, \tilde{x})]=\int_{x}v_{k-1}{*}(\Pi(\pi,x))f(x;\pi)dx$$


## Part 2.1-Example of

The model for $\theta$ is defined as:

$$\theta=Ap^*$$
Consumer’s uncertainty about $p*$ has a beta distribution

$$f(p^*) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}(p^*)^{\alpha-1}(1-p^*)^{\beta-1}$$
### Discussion on Beta Distribiution:

The pdf of the beta distribution, for 0<$p*$<1, and shape parameters $\alpha$,$\beta$>0, is a power function of the variable x and of its reflection (1-x) as follows:

$$f(x|\alpha,\beta) = constant.x^{\alpha-1}(1-x)^{\beta-1}$$
$$=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{\int_{0}^{1}u^{\alpha-1}(1-u)^{\beta-1}du}$$
$$=\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}$$
$$= \frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}$$
The beta function, B is a normalization constant to ensure that the total probability is 1.

### Example of Beta Distribution:

Example:

#### equal alpha and beta:

```{r}
p = seq(0,1,length.out = 100)
plot(p,dbeta(p,100,100),ylab = "density", type="l", col=4)
lines(p,dbeta(p,10,10), type = "l", col=3)
lines(p,dbeta(p,2,2), type = "l", col=2)
lines(p,dbeta(p,1,1), type = "l", col=1)
legend(0.7,8, c("Be(100,100)","Be(10,10)","Be(2,2)", "Be(1,1)"),col=c(4,3,2,1), lty = c(1,1,1,1) )

```
#### non-equal alpha and beta:

```{r}
p = seq(0,1, length=100)
plot(p, dbeta(p, 900, 100), ylab="density", type ="l", col=4)
lines(p, dbeta(p, 90, 10), type ="l", col=3)
lines(p, dbeta(p, 30, 70), col=2) 
lines(p, dbeta(p, 3, 7), col=1) 
legend(0.2,30, c("Be(900,100)","Be(90,10)","Be(30,70)", "Be(3,7)"),lty=c(1,1,1,1),col=c(4,3,2,1))
```

Two points about the *Beta* distribution:

From these examples you should note the following:
$$E[p]=\frac{\alpha}{\alpha+\beta}$$
* It turns out that the mean is exactly Thus the mean of the distribution is determined by the relative values of a and b.

* The larger the values of a and b, the smaller the variance of the distribution about the mean.

$$Var[p]=E[(X-\mu)^2]=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$$

#### alpha and beta <1


```{r}
p = seq(0,1, length=100)
plot(p, dbeta(p, 0.9, 0.1), ylab="density", type ="l", col=4)
lines(p, dbeta(p, 0.7, 0.3), type ="l", col=3)
lines(p, dbeta(p, 0.5, 0.7), col=2) 
lines(p, dbeta(p, 0.3, 0.9), col=1) 
legend(0.2,4, c("Be(0.9,0.1)","Be(0.7,0.3)","Be(0.5,0.7)", "Be(0.3,0.9)"), col=c(4,3,2,1), lty=c(1,1,1,1))
```

## Beta Conjugate to Bernoulli Distibutaion

### Prior:

$$Beta(\alpha,\beta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha,\beta)}$$

$$= const.\theta^{\alpha-1}(1-\theta)^{b-1}$$

### Posterior Distribution:

$$P(\theta|data)=\frac{P(data|\theta)P(\theta)}{P(data)}\propto P(data|\theta)P(theta)$$

## Beta-Binomial Model

### Likelihood:

$$P(data|\theta) \propto \theta^z(1-\theta)^{n-z},$$
if we let z number of $x_{i}$ with value 1, i.e, (seeing z success in n trial)

$$z=\sum_{i=1}^{n}X_i$$

### Derive Posterior

$$P(\theta|data) \propto ( \theta^z(1-\theta)^{n-z})(\theta^{\alpha-1}(1-\theta)^{\beta-1})$$       
$$\propto \theta^{z+\alpha-1}(1-\alpha)^{n-z+\beta-1}$$
Subsituing the following:

$$\alpha'=\alpha + z$$
$$\beta'=n+\beta-z$$
Now we have posterior:

$$P(\theta | data) = \frac{\theta^{\alpha'-1}(1-\alpha)^{\beta'-1}}{B(\alpha',\beta')}$$

$$=Beta(\alpha',\beta')$$

## Example of the @ulu2009or:


### Prior

Beta with $\alpha$=1, $\beta$=1:

```{r}

N <- 1000
alpha <- 1
beta <- 1
nsuccess <- 2
n_trials <- 4

data_1_1 <- tibble(p_grid = seq(from=0, to =1, length.out = N)) %>% 
  mutate(prior=dbeta(p_grid,alpha,beta)) %>% 
  mutate(likelihood = dbinom(nsuccess, size = n_trials, prob = p_grid)) %>% 
  mutate(posterior = (likelihood * prior) / sum(likelihood * prior))
```


```{r}

data_1_1 %>% 
  gather(key, value, -p_grid) %>%  
  # this line allows us to dictate the order the panels will appear in
  mutate(key = factor(key, levels = c("prior", "likelihood", "posterior"))) %>% 
  ggplot(aes(x = p_grid, ymin = 0, ymax = value, fill = key)) +
  geom_ribbon() +
  scale_fill_manual(values = c("blue", "red", "purple")) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(legend.position = "none") +
  facet_wrap(~key, scales = "free")
```

Analytically, the mean of the posterior should be:

$$Beta(\alpha + s, \beta + n -s)$$
with mean value of

$$E[X]=\frac{\alpha + s}{\alpha + \beta + n}$$
having $\alpha$=1, $\beta=1$, $s$=2 and $n$=4 :

$$Beta(1 + 2, 1+ 4-2)$$
$$Beta(3, 3)$$
 with mean value of 
 
$$E[pgrid]=\frac{3}{3+6}=\frac{1}{3}$$

## Normal-Normal Conjugate:

$$P(\theta | Data) = \frac{P(Data|\theta)P(\theta)}{P(Data)} \propto P(Data|\theta)P(\theta)$$
$$P(Data|\theta) = \prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma_x^2}}exp(-\frac{(x_i-\theta)²}{2\sigma_x^2})$$
$$\propto exp(-\frac{\sum_{i=1}^{N}(x_i-\theta)^2}{2\sigma_x^2})$$
Now posterior:

$$P(\theta|X) \propto  exp(-\frac{\sum_{i=1}^{N}(x_i-\theta)^2}{2\sigma_x^2})exp(-\frac{(\theta-\theta_0)^2}{2\sigma_{\theta}^2})$$
$$=exp[-$$
You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).

Figures and tables with captions will be placed in `figure` and `table` environments, respectively.

Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab).


You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and . change new line @jafarizadeh2020sja 
